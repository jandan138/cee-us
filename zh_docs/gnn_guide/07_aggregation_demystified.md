# 07 — 聚合（Aggregation）到底是什么：为什么要它、它在干什么、怎么选 mean/sum

如果你对“聚合”完全没概念，这是非常正常的：它看起来像个小细节（不就是 `sum/mean` 吗），但实际上它是 GNN 能工作的关键。

这一篇只讲三件事：

1. **为什么必须引入聚合**（不引入会怎样）
2. **聚合到底在聚合什么**（从“边消息”到“每个节点的一坨信息”）
3. **mean vs sum 怎么选**（对象数可变时为什么会出问题）

> 本文会尽量用“群聊/收件箱”的直觉讲清楚，然后再给一点点公式和 shape。

---

## 1. 先说结论：聚合是“把一堆邻居对我的影响，压缩成一个固定长度向量”

在 GNN 里，你通常会先对每条边算一个 **message（消息）**：

- “邻居 j 对我 i 的影响” → 一个向量 $m_{i\leftarrow j}$

问题来了：

- 节点 i 的邻居数量可能是 2 个、5 个、甚至 50 个
- 你最终要更新节点 i 的状态时，希望输入是**固定维度**的（这样 MLP 才能工作）

于是你需要一个操作：

- 输入：一堆 message 向量（数量可变）
- 输出：一个向量（维度固定）

这个操作就是 **Aggregation（聚合）**。

最常见的聚合是：

- `sum`：把所有消息向量逐元素求和
- `mean`：逐元素平均
- `max`：逐元素取最大

---

## 2. 为什么“必须”要聚合：因为邻居数量是可变的

### 2.1 不聚合会怎样？

假设你想更新节点 i，你得把它收到的所有 message 都喂给一个网络。

- 如果你把 message 直接拼起来（concatenate）：
  - 邻居 2 个 → 输入长度是 `2*d`
  - 邻居 5 个 → 输入长度是 `5*d`
  - 输入维度不固定，网络没法写成一个统一的 MLP

- 如果你强行规定“最多 K 个邻居”，多的截断、不足的补 0：
  - 你引入了一个非常生硬的规则（顺序怎么定？截断丢信息？）
  - 训练/测试对象数变化时更容易崩

**聚合的作用**就是让“邻居数量可变”这件事变得可处理：无论邻居多少，最后都变成一个固定维度向量。

### 2.2 更关键的一点：对象没有天然顺序

在很多环境里，“物体 1/2/3”这个编号只是数组下标，不代表真实世界的顺序。

如果你把邻居消息按某个顺序拼起来，模型会变得：

- **对顺序敏感**：同一批邻居，只是换了排列，输出就变了

而聚合（sum/mean/max）天然满足：

- **换顺序结果不变**（Permutation Invariant / 置换不变）

这就符合“对象集合没有顺序”的事实。

---

## 3. 聚合到底在聚合什么：从“边”回到“点”

### 3.1 一句话：边上算消息，点上收消息

在 message passing GNN 里通常是：

1. 对每条边 `(j -> i)` 计算消息 $m_{i\leftarrow j}$
2. 对每个节点 i，把所有发给 i 的消息聚合成 $\bar{m}_i$
3. 用 $\bar{m}_i$ 更新节点 i

你可以把每个节点想成一个“收件箱”：

- 边消息就是邮件
- 聚合就是把收件箱里的邮件总结成一段“摘要”

### 3.2 一个极小的数值例子（先不管维度）

节点 i 收到三条消息：

- 来自 A：`[10]`
- 来自 B：`[0]`
- 来自 C：`[0]`

聚合结果：

- `sum`：`[10]`
- `mean`：`[3.33]`
- `max`：`[10]`

这就是“邻居数量影响聚合结果”的根源：`mean` 会把影响摊薄，`sum` 会累积影响。

### 3.3 shape 视角：为什么代码里常见 segment_sum/mean

假设：

- batch 大小 `B`
- 每个样本有 `nObj` 个物体
- message 向量维度 `d_msg`

如果是全连接（去自环），边数大约是 `nObj*(nObj-1)`（有向）。

实现里经常把节点展平：

- 节点特征：`[B, nObj, d_node]` → flatten → `[B*nObj, d_node]`
- 边消息：对每条边算 → `[B*nEdge, d_msg]`

然后需要按“目标节点 id”把消息分桶求 `sum/mean`：

- 这就是 `segment_sum/segment_mean` 或者 `scatter_add` 这类操作
- 本质是：把属于同一个节点 i 的消息都加到一起（或平均）

你可以把它理解成：

- 有一个数组 `target_index` 指明每条边消息应该投递到哪个节点
- 聚合就是按 `target_index` 把消息放进每个节点的“收件箱”并做汇总

---

## 4. 置换不变性（Permutation Invariance）到底是什么意思

### 4.1 你真正想要的性质：只要“集合相同”，结果就相同

节点 i 的邻居是一组对象：`{A, B, C}`。

我们希望：

- 你按 A,B,C 的顺序喂给模型，得到的结果
- 和你按 C,A,B 的顺序喂给模型，得到的结果

应该一致。

这就是“置换不变”。

### 4.2 哪些操作满足置换不变

- `sum`
- `mean`
- `max`

哪些不满足：

- `concat`（拼接）
- `RNN` 按顺序读邻居

所以经典 GNN 结构里，聚合几乎一定用 sum/mean/max 这类集合函数。

---

## 5. mean vs sum：到底该怎么选（通俗解释）

这部分是你在本项目里最容易被 `aggr_fn` 卡住的点。

### 5.1 用“物理直觉”理解

- 如果你相信“受到的影响会随邻居数量增加而变大”，`sum` 更像“把所有力叠加”
- 如果你相信“邻居多了不代表影响一定更大，只是信息来源更多”，`mean` 更像“对邻居影响做归一化”

在对象数会变的场景里（训练 3 个物体，测试 5 个物体），常见问题是：

- 用 `sum` 时，消息规模会随邻居数量变大，网络可能在测试时输出偏大
- 用 `mean` 时，规模更稳定，但可能把“多物体叠加效应”摊薄

### 5.2 用数字再走一遍（你刚刚贴的例子）

节点 i 的两条消息：10 和 0

- `sum=10`
- `mean=5`

如果又多了两个 0（邻居变多但没带来新影响）：

- `sum` 还是 10
- `mean` 变 2.5

所以：

- `mean` 会让“邻居数量变化”显式影响结果（把非零消息平均稀释）
- `sum` 会让“邻居数量变化”隐式影响结果（非零消息不稀释，但如果新增邻居带来非零消息会叠加）

### 5.3 一个更贴近动力学的想象

- 如果多个物体同时接触某个物体，物理上合力可能确实叠加 → `sum` 有道理
- 但如果你全连接建图，很多边其实是“远距离、无接触”的，它们理论上应该贡献接近 0
  - 这时 `sum` 可能相对更稳（因为加了很多接近 0 的项也还是 0）
  - `mean` 可能会把少数有效交互“平均稀释”，尤其当图非常稠密时

这也是为什么你会看到有些实现会用 `sum`，但同时做额外的归一化（比如除以 degree、或者使用 attention 权重）。

---

## 6. 除了 mean/sum/max，还有更“聪明”的聚合吗？

有的，最常见的是 **attention 聚合**：

- 先给每条消息一个权重 $\alpha_{ij}$（可学习）
- 再做加权和：$\bar{m}_i = \sum_j \alpha_{ij} m_{ij}$

它依然是“集合 → 向量”的聚合，但更灵活：

- 让模型学会“哪些邻居更重要”
- 在全连接图里尤其有用（很多边应该被压到很小权重）

本项目当前设置里主要用 `mean/sum`（通过 `aggr_fn`），你先把这两种吃透就够读代码了。

---

## 7. 本项目里你应该怎么用这篇理解去读代码

你可以带着三个问题去看 GNN 的核心实现：

1. **message 是怎么从两端点算出来的？**（edge_mlp 输入拼了哪些东西）
2. **聚合是按什么 index 分桶的？**（是投递到 receiver 还是 sender）
3. **用的 mean 还是 sum，为什么？**（对应 config 的 `aggr_fn`）

如果你愿意，我也可以再补一篇“用最小 PyTorch 例子实现 segment_mean/segment_sum（含 target_index）”，你会对聚合瞬间更有感觉。
